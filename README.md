  # CSV to Parquet Data Pipeline with PySpark and PostgreSQL
     This project presents a robust data pipeline leveraging PySpark and PostgreSQL to process CSV data efficiently,
     store it in a PostgreSQL database, conduct SQL-based filtering operations, transform it into Parquet format, 
     and finally store the processed data back to an S3 bucket.

# Prerequisites
 - **Before you begin, ensure you have the following installed on your machine:**
    - [Python-3.7.0](https://www.python.org/downloads/release/python-370/)
    - [AWS CLI-2.15.18](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-version.html)
    - [Spark-3.5.0](https://spark.apache.org/downloads.html)
    - [Hadoop-3.2.1](https://hadoop.apache.org/release/3.2.1.html_)
    - [PostgreSQL-16.1](https://www.postgresql.org/download/windows/)
